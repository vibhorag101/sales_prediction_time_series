{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired from https://www.kaggle.com/code/dimitreoliveira/deep-learning-for-time-series-forecasting/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import optimizers\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, LSTM, RepeatVector, TimeDistributed, Flatten\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the train.csv file into a pandas dataframe\n",
    "df = pd.read_csv('TRAIN.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert the dates columns to a single column as 'date' with associated 'sales'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the dates columns to a single column as \"Date \" with associated sales\n",
    "df = pd.melt(df, id_vars=['Item code', 'Category', 'State'], var_name='date', value_name='sales')\n",
    "\n",
    "# Convert the 'Date' column to datetime format\n",
    "df['date'] = pd.to_datetime(df['date'], format='%d-%m-%Y')\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose dataframes with dates greater than 2022-02-01 and less than 2022-02-28\n",
    "df = df[(df['date'] >= '2021-07-01') & (df['date'] <= '2022-01-31')]\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the Item code, State and category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_dict = {}\n",
    "\n",
    "for col in ['Item code', 'State', 'Category']:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    le_dict[col] = le\n",
    "\n",
    "# Print the mappings\n",
    "for col, le in le_dict.items():\n",
    "    print(f\"For column {col}:\")\n",
    "    mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "    print(mapping)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print unique values of the 'Item code' column\n",
    "print(\"Unique values of the 'Item code' column:\")\n",
    "print(df['Item code'].unique())\n",
    "\n",
    "print(\"Unique values of the 'Category' column:\")\n",
    "print(df['Category'].unique())\n",
    "\n",
    "print(\"Unique values of the 'State' column:\")\n",
    "print(df['State'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualise this time series data using plotly\n",
    "fig = px.line(df, x='date', y='sales', title='Sales over time')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Min date from train set: %s' % df['date'].min().date())\n",
    "print('Max date from train set: %s' % df['date'].max().date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Percentage of entries with sales = 0: %s' % ((df['sales'] == 0).sum() / len(df) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the Item code vs sales\n",
    "fig = px.scatter(df, x='Item code', y='sales', title='Item code vs sales')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the Category vs sales\n",
    "fig = px.scatter(df, x='Category', y='sales', title='Category vs sales')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert data to training time series format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by date, item code, category and state and calculate the mean sales\n",
    "df = df.sort_values('date').groupby(['Item code','Category','State','date'], as_index=False)\n",
    "df = df.agg({'sales':['mean']})\n",
    "df.columns = ['Item code','Category','State','date','sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_supervised(data, window=1, lag=1, dropnan=True):\n",
    "    # cols stores the shifted dataframes\n",
    "    shifted_dataframes, shifted_column_names = list(), list()\n",
    "    # Input sequence (t-n, ... t-1)\n",
    "    for i in range(window, 0, -1):\n",
    "        # we are shifting the dataframe rows by i steps and are storing the new dataframes and there column names in a list\n",
    "        shifted_dataframes.append(data.shift(i))\n",
    "        shifted_column_names += [('%s(t-%d)' % (col, i)) for col in data.columns]\n",
    "    # Current timestep (t=0)\n",
    "    shifted_dataframes.append(data)\n",
    "    shifted_column_names += [('%s(t)' % (col)) for col in data.columns]\n",
    "    # Target timestep (t=lag)\n",
    "    shifted_dataframes.append(data.shift(-lag))\n",
    "    shifted_column_names += [('%s(t+%d)' % (col, lag)) for col in data.columns]\n",
    "    # Put it all together\n",
    "    agg = pd.concat(shifted_dataframes, axis=1)\n",
    "    agg.columns = shifted_column_names\n",
    "    # Drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the timeseries data into lagged features data with target sales after 90 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 59\n",
    "lag = 90\n",
    "series = series_to_supervised(df.drop('date', axis=1), window=window, lag=lag)\n",
    "print(series.shape)\n",
    "series.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avoid the scenario  when we are trying to predict the sale of a different product using different product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_Item_code = 'Item code(t-%d)' % window\n",
    "last_category = 'Category(t-%d)' % window\n",
    "last_state = 'State(t-%d)' % window\n",
    "\n",
    "series = series[(series['Item code(t)'] == series[last_Item_code])]\n",
    "series = series[(series['Item code(t+%d)'%lag] == series[last_Item_code])]\n",
    "series = series[(series['Category(t)'] == series[last_category])]\n",
    "series = series[(series['Category(t+%d)'%lag] == series[last_category])]\n",
    "series = series[(series['State(t)'] == series[last_state])]\n",
    "series = series[(series['State(t+%d)'%lag] == series[last_state])]\n",
    "\n",
    "print(series.shape)\n",
    "series.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop the Item code, Category and state columns as they are not needed to model time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [('%s(t+%d)' % (col, lag)) for col in ['Item code', 'Category', 'State']]\n",
    "for i in range(window, 0, -1):\n",
    "    columns_to_drop += [('%s(t-%d)' % (col, i)) for col in ['Item code', 'Category', 'State']]\n",
    "series.drop(columns_to_drop, axis=1, inplace=True)\n",
    "series.drop(['Item code(t)', 'Category(t)', 'State(t)'], axis=1, inplace=True)\n",
    "print(series.shape)\n",
    "series.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_col = 'sales(t+%d)' % lag\n",
    "labels = series[labels_col]\n",
    "series = series.drop(labels_col, axis=1)\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(series, labels.values, test_size=0.3, random_state=42)\n",
    "print('Train set shape', X_train.shape)\n",
    "print('Validation set shape', X_valid.shape)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_series = X_train.values.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_valid_series = X_valid.values.reshape((X_valid.shape[0], X_valid.shape[1], 1))\n",
    "print('Train set shape', X_train_series.shape)\n",
    "print('Validation set shape', X_valid_series.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch = 256\n",
    "lr = 0.0003\n",
    "adam = optimizers.Adam(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = Sequential()\n",
    "model_lstm.add(LSTM(50, activation='relu', input_shape=(X_train_series.shape[1], X_train_series.shape[2])))\n",
    "model_lstm.add(Dense(1))\n",
    "model_lstm.compile(loss='mse', optimizer=adam)\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# define the checkpoint\n",
    "filepath = \"best_model.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "# fit the model\n",
    "lstm_history = model_lstm.fit(X_train_series, Y_train, validation_data=(X_valid_series, Y_valid), epochs=epochs, verbose=2, callbacks=[checkpoint])\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(lstm_history.history['loss'])\n",
    "plt.plot(lstm_history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "\n",
    "# save the model to disk\n",
    "# model_lstm.save('model_lstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "from keras.models import load_model\n",
    "model_lstm = load_model('best_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_train_pred = model_lstm.predict(X_train_series)\n",
    "lstm_valid_pred = model_lstm.predict(X_valid_series)\n",
    "print('Train rmse:', mean_squared_error(Y_train, lstm_train_pred))\n",
    "print('Validation rmse:', mean_squared_error(Y_valid, lstm_valid_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wmape(y_true, y_pred):\n",
    "    total = 0\n",
    "    count = 0\n",
    "    for i in range(0, len(y_true)):\n",
    "        y_true_chunk = y_true[i]\n",
    "        y_pred_chunk = y_pred[i]\n",
    "        total += np.abs(y_true_chunk - y_pred_chunk)\n",
    "        count += np.abs(y_true_chunk)\n",
    "    return 100 * total / count\n",
    "\n",
    "print('Train WMAPE:', wmape(Y_train, lstm_train_pred)) \n",
    "print('Validation WMAPE:', wmape(Y_valid, lstm_valid_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autotrain-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
